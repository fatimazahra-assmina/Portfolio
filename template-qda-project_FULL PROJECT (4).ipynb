{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange\">Quality Data Analysis - Project Work [FULL PROJECT] </span>\n",
    "## Team 14\n",
    "\n",
    "### Course Details:\n",
    "- Academic Year: 2023-2024\n",
    "\n",
    "### Project Details:\n",
    "- Title: Design a statistical process monitoring method for the in-line detection of defects\n",
    "\n",
    "### Team Members:\n",
    "- Fatemeh Ahmadi\n",
    "- Fatima zahra Assmina\n",
    "- Josefina BraÃ±es\n",
    "- Bhavani Babu\n",
    "\n",
    "### Instructor:\n",
    "- Colosimo Bianca Maria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary data analysis\n",
    "Having the new dataset containing defective and non defective objects, we tried at first to visualize how is the new data we have to know how to deal with it before checking the assumptions to it as well as before plotting the control charts.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisrt of all, we began by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Import the library for hypothesis testing scipy\n",
    "import scipy.stats as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for hypothesis testing scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import the dataset\n",
    "data_raw = pd.read_excel('/Void Region Phase 2.xlsx')\n",
    "\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'Region type' is 'void'\n",
    "data_tr = data_raw.loc[data_raw['Region type'] == 'void']\n",
    "\n",
    "# Select rows where 'Region type' is 'part'\n",
    "data_part = data_raw.loc[data_raw['Region type'] == 'part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_tr.groupby(['Image name','Position','Region type'],as_index=False).mean()\n",
    "print(len(data))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove variables that are not useful\n",
    "data1 = data[['Area [pixels]', 'Perimeter [pixels]', 'Eccentricity', 'Solidity', 'Extent','Major Axis Length [pixels]', 'Minor Axis Length [pixels]', 'Equivalent Diameter [pixels]']]\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data void\n",
    "data_std = (data1 - data1.mean()) / data1.std()\n",
    "data_std.describe()\n",
    "data_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove variables that are not useful part\n",
    "data2 = data_part[['Area [pixels]', 'Perimeter [pixels]', 'Eccentricity', 'Solidity', 'Extent','Major Axis Length [pixels]', 'Minor Axis Length [pixels]', 'Equivalent Diameter [pixels]']]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data part\n",
    "data_std2 = (data2 - data2.mean()) / data2.std()\n",
    "data2.describe()\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std2 = data_std2.reset_index(drop=True, inplace=False)\n",
    "data_std2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data part\n",
    "fig, ax = plt.subplots(2, 3, figsize = (18, 8))\n",
    "ax[0,0].plot(data_std2 ['Area [pixels]'], 'o-')\n",
    "ax[0,0].set_title('Area')\n",
    "ax[0,1].plot(data_std2 ['Perimeter [pixels]'], 'o-')\n",
    "ax[0,1].set_title('Perimeter')\n",
    "ax[0,2].plot(data_std2 ['Eccentricity'], 'o-')\n",
    "ax[0,2].set_title('Eccentricity')\n",
    "ax[1,0].plot(data_std2 ['Solidity'], 'o-')\n",
    "ax[1,0].set_title('Solidity')\n",
    "ax[1,1].plot(data_std2 ['Major Axis Length [pixels]'], 'o-')\n",
    "ax[1,1].set_title('Major Axis Length [pixels]')\n",
    "ax[1,2].plot(data_std2 ['Minor Axis Length [pixels]'], 'o-')\n",
    "ax[1,2].set_title('Minor Axis Length [pixels]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data void\n",
    "fig, ax = plt.subplots(2, 3, figsize = (18, 8))\n",
    "ax[0,0].plot(data_std ['Area [pixels]'], 'o-')\n",
    "ax[0,0].set_title('Area')\n",
    "ax[0,1].plot(data_std ['Perimeter [pixels]'], 'o-')\n",
    "ax[0,1].set_title('Perimeter')\n",
    "ax[0,2].plot(data_std ['Eccentricity'], 'o-')\n",
    "ax[0,2].set_title('Eccentricity')\n",
    "ax[1,0].plot(data_std ['Solidity'], 'o-')\n",
    "ax[1,0].set_title('Solidity')\n",
    "ax[1,1].plot(data_std ['Major Axis Length [pixels]'], 'o-')\n",
    "ax[1,1].set_title('Major Axis Length [pixels]')\n",
    "ax[1,2].plot(data_std ['Minor Axis Length [pixels]'], 'o-')\n",
    "ax[1,2].set_title('Minor Axis Length [pixels]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyzed the data after plotting it and realized that it had the same performance as well as shape as the one we had in phase one.\n",
    "We plotted the data making the same difference made before in phase 1, the void and the part regions one apart from the other and then after applying the image processing code, we have seen the schemas for each one of the regions depending on the parameter or column we are referring to while plotting.\n",
    "This made us persue the same road as phase 1 and pushed us to go test the assumptions with the chosen features or parameters from the previous phase to validate them and then use the I_MR chart again, but this time to differentiate between real defective and non defective objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of your proposed approach on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by the Part region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "data = pd.read_excel('Part Region Phase 2.xlsx')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the data of the Solidity and Perimeter [pixels] used on the control charts for the part region to have a visual representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (18, 6))\n",
    "plt.tight_layout()\n",
    "ax[0].plot(data_std ['Solidity'], 'o-')\n",
    "ax[0].set_title('Solidity')\n",
    "ax[1].plot(data_std ['Perimeter [pixels]'], 'o-')\n",
    "ax[1].set_title('Perimeter [pixels]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borderline situation at Lag 2 of ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "n = len(data ['Solidity'])\n",
    "\n",
    "#Autocorrelation function\n",
    "[acf_values, lbq, _] = acf(data ['Solidity'], nlags = int(np.sqrt(n)), qstat=True, fft = False)\n",
    "\n",
    "#Bartlett's test at lag 2\n",
    "alpha = 0.05\n",
    "lag_test = 2\n",
    "rk = acf_values[lag_test]\n",
    "z_alpha2 = stats.norm.ppf(1-alpha/2)\n",
    "print('Test statistic rk = %f' % rk)\n",
    "print('Rejection region starts at %f' % (z_alpha2/np.sqrt(n)))\n",
    "\n",
    "if rk>z_alpha2/np.sqrt(n):\n",
    "    print('The null hypothesis is rejected: there is autocorrelation')\n",
    "else: print('The null hypothesis is accepted: there is no autocorrelation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked if the data are normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Shapiro-Wilk test\n",
    "_, pval_SW = stats.shapiro(data['Solidity'])\n",
    "print('Shapiro-Wilk test p-value = %.3f' % pval_SW)\n",
    "\n",
    "# Plot the qqplot\n",
    "stats.probplot(data ['Solidity'], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the p-value of the Shapiro-Wilks test is 0,6%, smaller than 5%, then data are not normal\n",
    "Therefore, we apply Box-Cox transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox transformation \n",
    "[data_BC, lmbda] = stats.boxcox(data['Solidity'])\n",
    "\n",
    "print('Lambda = %.3f' % lmbda)\n",
    "\n",
    "# Plot a histogram of the transformed data\n",
    "plt.hist(data_BC)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Shapiro-Wilk test to test normality\n",
    "_, p_value_SW = stats.shapiro(data_BC)\n",
    "print('p-value of the Shapiro-Wilk test: %.3f' % p_value_SW)\n",
    "\n",
    "# QQ-plot\n",
    "stats.probplot(data_BC, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the p-value is 5,4%, data are normal. However, we will use a lambda = 0 for Box-Cox transformation and check if we can obtain a p-value greater than with lambda = 30.921 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lambda = 0 \n",
    "data_BC1 = stats.boxcox(data['Solidity'], lmbda=0)\n",
    "# Plot a histogram of the transformed data\n",
    "plt.hist(data_BC1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the Shapiro-Wilk test\n",
    "_, p_value_SW = stats.shapiro(data_BC1)\n",
    "print('p-value of the Shapiro-Wilk test: %.3f' % p_value_SW)\n",
    "\n",
    "# QQ-plot\n",
    "stats.probplot(data_BC1, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that data are not normal after the Box-Cox transformation with lambda=0, we will use the data transformed with lambda = 30.921 and check autocorrelation once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "sgt.plot_acf(data_BC, lags = int(len(data)/3), zero=False, ax=ax[0])\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "sgt.plot_pacf(data_BC, lags = int(len(data)/3), zero=False, ax=ax[1], method = 'ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for Solidity are normal and not autocorrelated.\n",
    "We then redefine the data of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Solidity1=data_BC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF and PACF were plotted to analyze the Perimeter [pixels] feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "sgt.plot_acf(data['Perimeter [pixels]'], lags = int(len(data)/3), zero=False, ax=ax[0])\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "sgt.plot_pacf(data['Perimeter [pixels]'], lags = int(len(data)/3), zero=False, ax=ax[1], method = 'ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lags stay within the acceptance region, meaning that the data are not autocorrelated\n",
    "We also checked normality by performing the Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pval_SW = stats.shapiro(data['Perimeter [pixels]'])\n",
    "print('Shapiro-Wilk test p-value = %.3f' % pval_SW)\n",
    "\n",
    "# Plot the qqplot\n",
    "stats.probplot( data['Perimeter [pixels]'], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the p-value of the Shapiro-Wilk test is 63%, greater than 5%, we fail to reject the null hypothesis and the data are normal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we need the values of Solidity and Perimeter [pixels] to be at a similar scale than the ones from phase 1, we will standardize the data cosnidering the same features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the Soldity1 data to the initial data frame\n",
    "data['Solidity1'] = Solidity1\n",
    "data_reduced = data[['Area [pixels]', 'Perimeter [pixels]', 'Solidity1', 'Extent', 'Major Axis Length [pixels]', 'Minor Axis Length [pixels]']]\n",
    "\n",
    "data_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize\n",
    "data_std = (data_reduced - data_reduced.mean()) / data_reduced.std()\n",
    "data_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part=data_std[['Solidity1', 'Perimeter [pixels]']]\n",
    "data_part.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to apply the assumptions to the void region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the assumptions for the parameters: Major axis Lenght and Eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs test for randomness\n",
    "\n",
    "print(\"Eccentricity\")\n",
    "stat_runs, pval_runs = runstest_1samp(data1['Eccentricity'], correction=False)\n",
    "print('Runs test statistic = {:.3f}'.format(stat_runs))\n",
    "print('Runs test p-value = {:.3f}'.format(pval_runs))\n",
    "alfa=0.05\n",
    "if pval_runs < alfa:\n",
    "    print('Reject H0: the data are not random\\n')\n",
    "else:\n",
    "    print('Accept H0: the data are random\\n')\n",
    "\n",
    "\n",
    "print(\"Major axis\")\n",
    "stat_runs, pval_runs = runstest_1samp(data1['Major Axis Length [pixels]'], correction=False)\n",
    "print('Runs test statistic = {:.3f}'.format(stat_runs))\n",
    "print('Runs test p-value = {:.3f}'.format(pval_runs))\n",
    "alfa=0.05\n",
    "if pval_runs < alfa:\n",
    "    print('Reject H0: the data are not random\\n')\n",
    "else:\n",
    "    print('Accept H0: the data are random\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check ACF and PACF of Eccentricity\n",
    "\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "sgt.plot_acf(data_std['Eccentricity'], lags = int(len(data)/3), zero=False, ax=ax[0])\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "sgt.plot_pacf(data_std['Eccentricity'], lags = int(len(data)/3), zero=False, ax=ax[1], method = 'ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std['lag1'] = data_std['Eccentricity'].shift(1)\n",
    "sns.regplot(x=data_std['lag1'], y=data_std['Eccentricity'], ci=None, line_kws={'color':'red', 'ls':'--'})\n",
    "plt.title('Scatter plot of X(t-1) vs X(t)')\n",
    "plt.xlabel('X(t-1)')\n",
    "plt.ylabel('X(t)')\n",
    "plt.title('Scatter plot of X(t-1) vs X(t)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "n = len(data_std['lag1'][1:])\n",
    "\n",
    "#autocorrelation function\n",
    "[acf_values, lbq, _] = acf(data_std['lag1'][1:], nlags = int(np.sqrt(n)), qstat=True, fft = False)\n",
    "\n",
    "#Bartlett's test at lag 1\n",
    "alpha = 0.05\n",
    "lag_test = 1\n",
    "rk = acf_values[lag_test]\n",
    "z_alpha2 = stats.norm.ppf(1-alpha/2)\n",
    "print('Test statistic rk = %f' % rk)\n",
    "print('Rejection region starts at %f' % (z_alpha2/np.sqrt(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Shapiro-Wilk test\n",
    "_, pval_SW = stats.shapiro(data_std['lag1'][1:])\n",
    "print('Shapiro-Wilk test p-value = %.3f' % pval_SW)\n",
    "\n",
    "# Plot the qqplot\n",
    "stats.probplot(data_std['lag1'][1:], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA model (1,1,1) import the necessary library\n",
    "import qda\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "x = data['Eccentricity']\n",
    "model = qda.ARIMA(x, order=(1,1,1), add_constant=True)\n",
    "\n",
    "qda.ARIMAsummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA model (1,1,1) without the constant\n",
    "\n",
    "x = data['Eccentricity']\n",
    "model = qda.ARIMA(x, order=(1,1,1), add_constant=False)\n",
    "\n",
    "qda.ARIMAsummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we observe that p value of LBQ are all higher than 5%, meaning we do not expect to see any kind of autocorrelation on the residuals\n",
    "\n",
    "#Extract the residuals\n",
    "residuals_area = model.resid[1:]\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "_, pval_SW = stats.shapiro(residuals_area)\n",
    "print('Shapiro-Wilk test p-value = %.3f' % pval_SW)\n",
    "\n",
    "# Plot the qqplot\n",
    "stats.probplot(residuals_area, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers and check normality again\n",
    "data_out_area = residuals_area.drop(index=2)\n",
    "\n",
    "#Shapiro-Wilk test\n",
    "from scipy.stats import shapiro\n",
    "stat_shapiro_out, p_shapiro_out = shapiro(data_out_area)\n",
    "print('Statistic = %.3f, p-val = %.3f' % (stat_shapiro_out, p_shapiro_out))\n",
    "\n",
    "#Interpretation\n",
    "alpha = 0.05\n",
    "if p_shapiro_out > alpha:\n",
    "    print('Fail to reject H0')\n",
    "else:\n",
    "    print('Reject H0')\n",
    "\n",
    "#Plot the qqplot\n",
    "stats.probplot(data_out_area, dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check ACF and PACF of Major Axis Length [pixels]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "sgt.plot_acf(data_std['Major Axis Length [pixels]'], lags = int(len(data)/3), zero=False, ax=ax[0])\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "sgt.plot_pacf(data_std['Major Axis Length [pixels]'], lags = int(len(data)/3), zero=False, ax=ax[1], method = 'ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data_std['Major Axis Length [pixels]'])\n",
    "\n",
    "#autocorrelation function\n",
    "[acf_values, lbq, _] = acf(data_std['Major Axis Length [pixels]'], nlags = int(np.sqrt(n)), qstat=True, fft = False)\n",
    "\n",
    "#Bartlett's test at lag 4\n",
    "alpha = 0.05\n",
    "lag_test = 4\n",
    "rk = acf_values[lag_test]\n",
    "z_alpha2 = stats.norm.ppf(1-alpha/2)\n",
    "print('Test statistic rk = %f' % rk)\n",
    "print('Rejection region starts at %f' % (z_alpha2/np.sqrt(n)))\n",
    "\n",
    "if rk>z_alpha2/np.sqrt(n):\n",
    "    print('The null hypothesis is rejected')\n",
    "else: print('The null hypothesis is accepted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply difference operator\n",
    "data_std['lag1'] = data_std['Major Axis Length [pixels]'].shift(1)\n",
    "sns.regplot(x=data_std['lag1'], y=data_std['Major Axis Length [pixels]'], ci=None, line_kws={'color':'red', 'ls':'--'})\n",
    "plt.title('Scatter plot of X(t-1) vs X(t)')\n",
    "plt.xlabel('X(t-1)')\n",
    "plt.ylabel('X(t)')\n",
    "plt.title('Scatter plot of X(t-1) vs X(t)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the difference between the data and lag1\n",
    "data_std['diff1'] = data_std['Major Axis Length [pixels]'] - data_std['lag1']\n",
    "plt.plot(data_std['diff1'], 'o-')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('DIFF 1')\n",
    "plt.title('Time series plot of DIFF 1')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check runs test again and also plot ACF and PACF\n",
    "_, pval_runs = runstest_1samp(data_std['diff1'][1:], correction=False)\n",
    "print('Runs test p-value = {:.3f}'.format(pval_runs))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "sgt.plot_acf(data_std['diff1'][1:], lags = int(len(data)/3), zero=False, ax=ax[0])\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "sgt.plot_pacf(data_std['diff1'][1:], lags = int(len(data)/3), zero=False, ax=ax[1], method = 'ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Shapiro-Wilk test\n",
    "_, pval_SW = stats.shapiro(data_std['diff1'][1:])\n",
    "print('Shapiro-Wilk test p-value = %.3f' % pval_SW)\n",
    "\n",
    "# Plot the qqplot\n",
    "stats.probplot(data_std['diff1'][1:], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the assumptions and having our new data really fit to the model and validating the assumptions for the chosen parameters in different regions part and void"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move to the last section which is testing our I-MR control chart and its ability to be efficient for our new data containing defective and non defective objects\n",
    "\n",
    "Let's see if it's going to generate alarms based on the features chosen whenever the object is defective!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Import the library for hypothesis testing scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import the dataset\n",
    "data_raw = pd.read_csv('data_qda.csv')\n",
    "\n",
    "data_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'Region type' is 'void'\n",
    "# in the following, since void regions are the small part related to the one picture, we need to grouped them and use the mean of all void regions related to each picture as single data for each sample\n",
    "data_void0 = data_raw.loc[data_raw['Region type'] == 'void']\n",
    "\n",
    "# Select rows where 'Region type' is 'part'\n",
    "data_par = data_raw.loc[data_raw['Region type'] == 'part']\n",
    "data_part = data_par.reset_index(drop=True, inplace=False)\n",
    "\n",
    "data_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the mean for each samples as single data based on the intial assumption\n",
    "\n",
    "data_void = data_void0.groupby(['Image name','Position','Region type'],as_index=False).mean()\n",
    "print(len(data_void))\n",
    "data_void.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select variables that are useful after assumption was satisfied\n",
    "data = data_void[['Eccentricity','Major Axis Length [pixels]' ]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as the plotted chart shows, the data are stationary with stable mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data void\n",
    "data_std = (data - data.mean()) / data.std()\n",
    "data_std.describe()\n",
    "data_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into two parts, one is for designing and the other for cheching the designed control chart\n",
    "def split_by_index(data_std):\n",
    "  \"\"\"Splits data into halves based on index.\n",
    "\n",
    "  Args:\n",
    "      data: A list containing the data.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing two lists: control_data and new_data.\n",
    "  \"\"\"\n",
    "  data_length = len(data_std)\n",
    "  midpoint = data_length // 2\n",
    "  control_data = data_std[:midpoint]\n",
    "  new_data = data_std[midpoint:]\n",
    "  return control_data, new_data\n",
    "\n",
    "control_data, new_data = split_by_index(data_std.copy())  # Copy data to avoid modifying original\n",
    "\n",
    "\n",
    "print(\"Control Data:\", control_data)\n",
    "print(\"New Data:\", new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    ">\n",
    ">As we have single data for each sample we are going to use I_MR control chart:\n",
    ">\n",
    "> **I chart**:\n",
    "> - $UCL = \\bar{x} + 3 \\left(\\frac{\\bar{MR}}{d_2} \\right)$\n",
    "> - $CL = \\bar{x}$\n",
    "> - $LCL = \\bar{x} - 3 \\left(\\frac{\\bar{MR}}{d_2} \\right)$\n",
    ">\n",
    "> **MR chart**:\n",
    "> - $UCL = D_4 \\bar{MR}$\n",
    "> - $CL = \\bar{MR}$\n",
    "> - $LCL = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the moving ranges using the diff function\n",
    "df = control_data.copy()\n",
    "df['MR1'] = df['Eccentricity'].diff().abs()  #MR1 for (Eccentrecity)\n",
    "df['MR2'] = df['Major Axis Length [pixels]'].diff().abs() #MR2 for (major axis)\n",
    "\n",
    "# Print out descriptive statistics of MR and time\n",
    "df.head()\n",
    "#data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the control limits void\n",
    "d2 = 1.128\n",
    "D4 = 3.267\n",
    "\n",
    "# make a copy of the data  #df is dataframe for void region\n",
    "#df = control_data.copy()\n",
    "# change the name of the column time to I\n",
    "df.rename(columns={'Eccentricity':'I1'}, inplace=True)\n",
    "df.rename(columns={'Major Axis Length [pixels]':'I2'}, inplace=True)\n",
    "\n",
    "# Print the first 5 rows of the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for the upper and lower control limits I1 (Eccentrecity)\n",
    "df['I1_UCL'] = df['I1'].mean() + (3*df['MR1'].mean()/d2)\n",
    "df['I1_CL'] = df['I1'].mean()\n",
    "df['I1_LCL'] = df['I1'].mean() - (3*df['MR1'].mean()/d2)\n",
    "df['MR1_UCL'] = D4 * df['MR1'].mean()\n",
    "df['MR1_CL'] = df['MR1'].mean()\n",
    "df['MR1_LCL'] = 0\n",
    "\n",
    "# Create columns for the upper and lower control limits  I2 (major axis)\n",
    "df['I2_UCL'] = df['I2'].mean() + (3*df['MR2'].mean()/d2)\n",
    "df['I2_CL'] = df['I2'].mean()\n",
    "df['I2_LCL'] = df['I2'].mean() - (3*df['MR2'].mean()/d2)\n",
    "df['MR2_UCL'] = D4 * df['MR2'].mean()\n",
    "df['MR2_CL'] = df['MR2'].mean()\n",
    "df['MR2_LCL'] = 0\n",
    "\n",
    "\n",
    "# Print the first 5 rows of the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for possible violations of the control limits I1 (Eccentrecity)\n",
    "df['I1_TEST1'] = np.where((df['I1'] > df['I1_UCL']) |\n",
    "                (df['I1'] < df['I1_LCL']), df['I1'], np.nan)\n",
    "df['MR1_TEST1'] = np.where((df['MR1'] > df['MR1_UCL']) |\n",
    "                (df['MR1'] < df['MR1_LCL']), df['MR1'], np.nan)\n",
    "\n",
    "# Define columns for possible violations of the control limits I2 (major axis)\n",
    "df['I2_TEST2'] = np.where((df['I2'] > df['I2_UCL']) |\n",
    "                (df['I2'] < df['I2_LCL']), df['I2'], np.nan)\n",
    "df['MR2_TEST2'] = np.where((df['MR2'] > df['MR2_UCL']) |\n",
    "                (df['MR2'] < df['MR2_LCL']), df['MR2'], np.nan)\n",
    "# Print the first 5 rows of the new dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for the first phase, No out of control data points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "The control charts, designed for void and part regions with their features, showed stable processes in both initial (phase 1) and ongoing monitoring (phase 2). While no data points breached control limits, an observed pattern within those limits couldn't be definitively explained due to our initial assumption of time-ordered data. Capturing time-ordered data in the future would allow us to investigate this pattern and identify potential causes for process variations within the control limits. \n",
    "For the future, We are going to use this chart for the second phase of the project to check if this chart is valid to identify defected object or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qda\n",
    "data_IMR = qda.ControlCharts.IMR(control_data, 'Eccentricity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chart above, there is no out of control data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qda\n",
    "data_IMR = qda.ControlCharts.IMR(control_data, 'Major Axis Length [pixels]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data for phase 2\n",
    "new_obs = pd.read_csv('data_qda2.csv')\n",
    "# Select rows where 'Region type' is 'void'\n",
    "# in the following, since void regions are the small part related to the one picture, we need to grouped them and use the mean of all void regions related to each picture as single data for each sample\n",
    "data_void02 = new_obs.loc[new_obs['Region type'] == 'void']\n",
    "\n",
    "# Select rows where 'Region type' is 'part'\n",
    "data_par02 = new_obs.loc[new_obs['Region type'] == 'part']\n",
    "data_part02 = data_par02.reset_index(drop=True, inplace=False)\n",
    "\n",
    "data_part02.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_void12 = data_void02.groupby(['Image name','Position','Region type'],as_index=False).mean()\n",
    "print(len(data_void12))\n",
    "data_void12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select variables that are useful after assumption was satisfied\n",
    "datav2 = data_void12[['Eccentricity','Major Axis Length [pixels]' ]]\n",
    "\n",
    "datav2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data void\n",
    "data_stdv2 = (datav2 - datav2.mean()) / datav2.std()\n",
    "data_stdv2.describe()\n",
    "data_stdv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datap2=data_part02[['Solidity','Perimeter [pixels]' ]]\n",
    "\n",
    "datap2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data void\n",
    "data_stdp2 = (datap2 - datap2.mean()) / datap2.std()\n",
    "data_stdp2.describe()\n",
    "data_stdp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the array with the new observations\n",
    "\n",
    "# Add the new observations to the dataset\n",
    "new_datav1 = pd.concat([control_data[['Eccentricity']], pd.DataFrame(data_stdv2, columns=['Eccentricity'])], ignore_index=True)\n",
    "new_datav2 = pd.concat([control_data[['Major Axis Length [pixels]']], pd.DataFrame(data_stdv2, columns=['Major Axis Length [pixels]'])], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "new_datav1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before we splited the data to the control data and the new data that the new data is assumed as new observation to check the control chart\n",
    "#new obs for Eccentricity\n",
    "new_data_IMR1 = qda.ControlCharts.IMR(new_datav1, 'Eccentricity', subset_size=len(control_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new obs for Major Axis Length [pixels]\n",
    "new_data_IMR2 = qda.ControlCharts.IMR(new_datav2, 'Major Axis Length [pixels]', subset_size=len(control_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chart above, there is no out of control data points, there is a point close to the lower control limits  which is in control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# part Region #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select variables that are  useful\n",
    "data_part1 = data_part[['Solidity','Perimeter [pixels]']]\n",
    "\n",
    "data_part1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data\n",
    "data_stdp = (data_part1 - data_part1.mean()) / data_part1.std()\n",
    "data_stdp.describe()\n",
    "data_stdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into two parts, one is for designing and the other for cheching the designed control chart\n",
    "\n",
    "def split_by_index(data_stdp):\n",
    "  \"\"\"Splits data into halves based on index.\n",
    "\n",
    "  Args:\n",
    "      data: A list containing the data.\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing two lists: control_data and new_data.\n",
    "  \"\"\"\n",
    "  data_lengthp = len(data_stdp)\n",
    "  midpointp = data_lengthp // 2\n",
    "  control_datap = data_stdp[:midpointp]\n",
    "  new_datap = data_stdp[midpointp:]\n",
    "  return control_datap, new_datap\n",
    "\n",
    "control_datap, new_datap = split_by_index(data_stdp.copy())  # Copy data to avoid modifying original\n",
    "\n",
    "# Use control_data and new_data for your purposes\n",
    "\n",
    "print(\"Control Data:\", control_datap)\n",
    "print(\"New Data:\", new_datap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the moving ranges using the diff function #control_datap\n",
    "dfp = control_datap.copy()\n",
    "\n",
    "dfp['MR1'] = dfp['Solidity'].diff().abs()\n",
    "dfp['MR2'] = dfp['Perimeter [pixels]'].diff().abs()\n",
    "\n",
    "\n",
    "# Print out descriptive statistics of MR and time\n",
    "dfp.head()\n",
    "#data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the control limits part\n",
    "d2 = 1.128\n",
    "D4 = 3.267\n",
    "\n",
    "# make a copy of the data\n",
    "# change the name of the column time to I\n",
    "dfp.rename(columns={'Solidity':'I1'}, inplace=True)\n",
    "dfp.rename(columns={'Perimeter [pixels]':'I2'}, inplace=True)\n",
    "\n",
    "# Print the first 5 rows of the new dataframe\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfp is dataframe for part region\n",
    "# Create columns for the upper and lower control limits I1 (Solidity)\n",
    "dfp['I1_UCL'] = dfp['I1'].mean() + (3*dfp['MR1'].mean()/d2)\n",
    "dfp['I1_CL'] = dfp['I1'].mean()\n",
    "dfp['I1_LCL'] = dfp['I1'].mean() - (3*dfp['MR1'].mean()/d2)\n",
    "dfp['MR1_UCL'] = D4 * dfp['MR1'].mean()\n",
    "dfp['MR1_CL'] = dfp['MR1'].mean()\n",
    "dfp['MR1_LCL'] = 0\n",
    "\n",
    "# Create columns for the upper and lower control limits  I2 (Perimeter)\n",
    "dfp['I2_UCL'] = dfp['I2'].mean() + (3*dfp['MR2'].mean()/d2)\n",
    "dfp['I2_CL'] = dfp['I2'].mean()\n",
    "dfp['I2_LCL'] = dfp['I2'].mean() - (3*dfp['MR2'].mean()/d2)\n",
    "dfp['MR2_UCL'] = D4 * dfp['MR2'].mean()\n",
    "dfp['MR2_CL'] = dfp['MR2'].mean()\n",
    "dfp['MR2_LCL'] = 0\n",
    "\n",
    "\n",
    "# Print the first 5 rows of the new dataframe\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns for possible violations of the control limits I1 (Solidity)\n",
    "dfp['I1_TEST1'] = np.where((dfp['I1'] > dfp['I1_UCL']) |\n",
    "                (dfp['I1'] < dfp['I1_LCL']), dfp['I1'], np.nan)\n",
    "dfp['MR1_TEST1'] = np.where((dfp['MR1'] > dfp['MR1_UCL']) |\n",
    "                (dfp['MR1'] < dfp['MR1_LCL']), dfp['MR1'], np.nan)\n",
    "\n",
    "# Define columns for possible violations of the control limits I2 (Perimeter)\n",
    "dfp['I2_TEST2'] = np.where((dfp['I2'] > dfp['I2_UCL']) |\n",
    "                (dfp['I2'] < dfp['I2_LCL']), dfp['I2'], np.nan)\n",
    "dfp['MR2_TEST2'] = np.where((dfp['MR2'] > dfp['MR2_UCL']) |\n",
    "                (dfp['MR2'] < dfp['MR2_LCL']), dfp['MR2'], np.nan)\n",
    "# Print the first 5 rows of the new dataframe\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "The control charts, designed for void and part regions with their features, showed stable processes in both initial (phase 1) and ongoing monitoring (phase 2). While no data points breached control limits, an observed pattern within those limits couldn't be definitively explained due to our initial assumption of time-ordered data. Capturing time-ordered data in the future would allow us to investigate this pattern and identify potential causes for process variations within the control limits. \n",
    "For the future, We are going to use this chart for the second phase of the project to check if this chart is valid to identify defected object or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qda\n",
    "data_IMR = qda.ControlCharts.IMR(control_datap, 'Solidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qda\n",
    "data_IMR = qda.ControlCharts.IMR(control_datap, 'Perimeter [pixels]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the chart above, all points are in control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the array with the new observations\n",
    "\n",
    "# Add the new observations to the dataset\n",
    "new_datap1 = pd.concat([control_datap[['Solidity']], pd.DataFrame(data_stdp2, columns=['Solidity'])], ignore_index=True)\n",
    "new_datap2 = pd.concat([control_datap[['Perimeter [pixels]']], pd.DataFrame(data_stdp2, columns=['Perimeter [pixels]'])], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new obs\n",
    "new_data_IMRp1 = qda.ControlCharts.IMR(new_datap1, 'Solidity', subset_size=len(control_datap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the chart above, all points are in control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new observation\n",
    "new_data_IMRp2 = qda.ControlCharts.IMR(new_datap2, 'Perimeter [pixels]', subset_size=len(control_datap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the chart above, all points are in control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's true that our new data provided for this phase 2 of the project had the same performances as the old one taken in M.A.D.E. when we plotted it. So we went for the assumptions checking the ones that were validated in the phase 1 for each region: the void and the part. \n",
    "The assumptions were also perfectly met and validated for the new data so we went right through plotting the control chart which is the I-MR as we are working with n=1 data with all assumptions met. The control chart were fit for the void and part region but there was a problem, the UCL and LCL  were far from thr data fluctuations in the control chart and our data didn't contain any NID data, so our model of monitoring wasn't perfectly met as it didn't show when the object is defective or not and didn't generate alarms when it was the case fo non defective.\n",
    "\n",
    "We have tried to test it for other features but it wasn't possible as they didn't meet the assumptions so we tried to omit the standardization of data to make it more fluctuous and near to the limits but it also didn't work. \n",
    "\n",
    "So we went for a completely new different way of analyzing the data and pollting it control chart relying on the residuals from a PCA  analysis to see if the object is defective or not, and here is what we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape for image\n",
    "# The larger the more computational time is required\n",
    "m = 230\n",
    "n = 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "# Split them in defect and defect-free\n",
    "path = '../data/processed/train/'\n",
    "defect = []\n",
    "defect_free = []\n",
    "rejected = []\n",
    "target = list(pd.read_csv('data_qda2.csv', sep = ';').Target)\n",
    "\n",
    "for f in range(1,51):\n",
    "    img = cv.imread(path + str(f) + '.bmp', cv.IMREAD_GRAYSCALE)\n",
    "    img = cv.resize(img, [m, n])\n",
    "    if target[f-1] == 0:\n",
    "        defect_free.append(img)\n",
    "    elif target[f-1] == 1:\n",
    "        defect.append(img)\n",
    "    else:\n",
    "        rejected.append(img)\n",
    "        \n",
    "print(\"Rejected:\", len(rejected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(img):\n",
    "    denoised = denoise(img)\n",
    "    blurred = blur_it(denoised, 5, 3, 3)\n",
    "    bw = binarize_it(blurred)\n",
    "    return bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_bw = []\n",
    "defect_free_bw = []\n",
    "\n",
    "for img in defect:\n",
    "    defect_bw.append(pipeline(img))\n",
    "\n",
    "for img in defect_free:\n",
    "    defect_free_bw.append(pipeline(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defect images\n",
    "fig, ax = plt.subplots(nrows = 4, ncols = 5, figsize = (15,15))\n",
    "plt.tight_layout()\n",
    "for a, i in zip (ax.flat, defect_bw):\n",
    "    a.imshow(i, cmap = 'gray')\n",
    "fig.suptitle(\"Defective Binarized Images\", fontproperties = {'size':24, 'family':'serif'}, y=1.005);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defect-free images\n",
    "fig, ax = plt.subplots(nrows = 6, ncols = 5, figsize = (15,15))\n",
    "plt.tight_layout()\n",
    "for a, i in zip (ax.flat, defect_free_bw):\n",
    "    a.imshow(i, cmap = 'gray')\n",
    "fig.suptitle(\"Defect-free Images\", fontproperties = {'size':24, 'family':'serif'}, y = 1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to make Principal Components Analysis in order to get a general defect-free piece.\n",
    "\n",
    "To identify defects in images, we will conduct a Principal Component Analysis (PCA) to determine the \"eigen-fancovers\" for defect-free image spaces. We will then retain a subset of the principal components (PCs). Using these selected PCs, we will reconstruct each image in the datasetâboth defective and defect-freeâas a linear combination of these \"eigen-fancovers.\" This reconstruction will represent the \"defect-free\" version of each piece. By subtracting the reconstructed image from the original image, we aim to highlight any defects present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the matrix\n",
    "matrix = np.empty(shape = (m*n, 30))\n",
    "for i in range(30):\n",
    "    arr = defect_free_bw[i].reshape((m*n,), order = 'C')\n",
    "    matrix[:,i] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double-check matrix shape\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display avg_fanc used to center the data\n",
    "avg_fanc = np.mean(matrix, axis =1)\n",
    "plt.imshow(avg_fanc.reshape((m,n), order = 'C'), cmap = 'gray')\n",
    "plt.title(\"Average fancover\", fontdict={'size':16, 'family':'serif'}, pad = 8);# Perform centered SVD i.e. PCA\n",
    "X = matrix - np.tile(avg_fanc, (matrix.shape[1],1)).T\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices = 0)\n",
    "\n",
    "# Save for reproducibility\n",
    "np.save(\"./modelPCA/u.npy\", U)\n",
    "np.save(\"./modelPCA/s.npy\", S)\n",
    "np.save(\"./modelPCA/vt.npy\", Vt)\n",
    "np.save(\"./modelPCA/avg_fanc.npy\", avg_fanc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_plot = []\n",
    "for i in S.flatten():\n",
    "    s_plot.append(i)\n",
    "\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "plt.plot(s_plot, color = 'steelblue', lw = 2, marker = 'o', markersize = 10, mfc = 'w' )\n",
    "plt.ylabel(\"$\\sigma_i$\")\n",
    "plt.xlim(xmin = -2, xmax = 30)\n",
    "plt.title(\"Singular value of $\\Sigma$\", fontdict = {'family': 'serif','size': 16})\n",
    "plt.savefig(\"./images/sing_value_s_abs.png\", dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_perc_plot = []\n",
    "for i in S.flatten():\n",
    "    s_perc_plot.append(i/np.sum(S.flatten()))\n",
    "\n",
    "cum_explained_var=np.cumsum(s_perc_plot)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "a = plt.bar(np.arange(0,30), s_perc_plot, width = .8, color = \"#D2D2D2\", edgecolor = 'k')\n",
    "plt.ylabel('Explained energy')\n",
    "ax2 = plt.twinx()\n",
    "ax2 = plt.hlines(y=0.5, xmin=-1, xmax = 30, color = 'r', ls = '--', lw = 1)\n",
    "plt.text(29.2, 0.495, \"0.5\", fontdict={\"color\":'r', \"weight\":\"bold\"})\n",
    "ax2 = plt.hlines(y=0.75, xmin=-1, xmax = 30, color = 'r', ls = '--', lw = 1)\n",
    "plt.text(29.2, 0.745, \"0.75\", fontdict={\"color\":'r', \"weight\":\"bold\"})\n",
    "ax2 = plt.hlines(y=0.95, xmin=-1, xmax = 30, color = 'r', ls = '--', lw = 1)\n",
    "plt.text(29.2, 0.945, \"0.95\", fontdict={\"color\":'r', \"weight\":\"bold\"})\n",
    "ax2 = plt.plot(np.arange(0,29), cum_explained_var[:-1], color = 'steelblue', marker = 'o', markersize = 8, markeredgewidth=2, mfc = 'w')\n",
    "plt.xlim(-1,29)\n",
    "plt.ylabel(\"Cumsum\")\n",
    "plt.title(\"Explained energy vs cumulative explained energy\", fontdict = {'family': 'serif','size': 16});\n",
    "plt.savefig(\"./images/sing_val_rel_cum.png\", dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_plot_norm = (s_plot - np.mean(s_plot))/np.std(s_plot)\n",
    "marginal_gain_var = [s_plot_norm[i] - s_plot_norm[i+1] for i in range(len(s_plot_norm)-1)]\n",
    "\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "plt.plot(marginal_gain_var, color = 'steelblue', lw = 2, marker = 'o', markersize = 10, mfc = 'w')\n",
    "plt.ylabel(\"$\\Delta _ {\\sigma^2}$\")\n",
    "plt.hlines(y=0.1, xmin=-.5, xmax=29, color = 'r', linestyle = '--', lw =1)\n",
    "plt.text(-1.3, 0.095, \"0.1\", fontdict={\"color\":'r', \"size\":11})\n",
    "\n",
    "plt.hlines(y=0.25, xmin=-.5, xmax=29, color = 'r', linestyle = '--', lw =1)\n",
    "plt.text(-1.5, 0.245, \"0.25\", fontdict={\"color\":'r', \"size\":11})\n",
    "plt.xlim(xmin = -.5, xmax = 29)\n",
    "\n",
    "plt.hlines(y=0.5, xmin=-.5, xmax=29, color = 'r', linestyle = '--', lw =1)\n",
    "plt.text(-1.3, 0.495, \"0.5\", fontdict={\"color\":'r', \"size\":11})\n",
    "plt.xlim(xmin = -.5, xmax = 29)\n",
    "\n",
    "plt.title(\"Marginal gain of explained variance\", fontdict = {'family': 'serif','size': 16});\n",
    "plt.savefig(\"./images/marginal_energy.png\", dpi = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to make a considered choice: if we choose to keep too many images, the result will be very sensitive to variations (noise, color, and so on and so forth) that may be in the results. Therefore, we decided to sector as a threshold 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_img = []\n",
    "for i in range(4):\n",
    "    eigen_img.append(np.reshape(U[:,i], (m,n)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (10,8))\n",
    "plt.tight_layout()\n",
    "for j, (i, a) in enumerate(zip(eigen_img, ax.flatten())):\n",
    "    a.imshow(i, cmap = 'gray')\n",
    "    a.set_title(\"Eigen Image \" + str(j+1), fontdict={'size':16, 'family':'serif'})\n",
    "ax[1][1].imshow(np.reshape(avg_fanc, (m,n)), cmap = 'gray')\n",
    "ax[1][1].set_title('Avg fan-cover', fontdict={'size':16, 'family':'serif'})\n",
    "ax[1][2].set_axis_off()\n",
    "plt.savefig('./images/eigen_images_five.png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_img = defect_bw[0]\n",
    "plt.imshow(def_img, cmap = 'gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the defect-freedom reconstruction\n",
    "rank = 4\n",
    "def_mc = def_img.reshape((m*n,), order = 'C') - avg_fanc\n",
    "partial = U[:,:rank].T @ def_mc\n",
    "rec =avg_fanc + U[:,:rank] @ partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize = (10,10))\n",
    "plt.tight_layout()\n",
    "ax[0].imshow(def_img, cmap = 'gray')\n",
    "ax[1].imshow(rec.reshape((m,n), order = 'C'), cmap = 'gray')\n",
    "ax[0].set_title(\"Original image\", fontdict={'size':16, 'family':'serif'})\n",
    "ax[1].set_title(\"Defect-freedom of the image\", fontdict={'size':16, 'family':'serif'})\n",
    "plt.savefig(\"./images/defect_freedom.png\", dpi = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find residuals\n",
    "rec = rec.reshape((m,n), order = 'C')\n",
    "residual = rec - def_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize = (10,10))\n",
    "plt.tight_layout()\n",
    "ax[0].imshow(def_img, cmap = 'gray')\n",
    "ax[1].imshow(rec.reshape((m,n), order = 'C'), cmap = 'gray')\n",
    "ax[2].imshow(residual.reshape((m,n), order = 'C'), cmap = 'gray')\n",
    "ax[0].set_title(\"Original image\", fontdict={'size':14, 'family':'serif'})\n",
    "ax[1].set_title(\"Defect-freedom of the image\", fontdict={'size':14, 'family':'serif'})\n",
    "ax[2].set_title(\"Residuals\", fontdict={'size':14, 'family':'serif'})\n",
    "\n",
    "plt.savefig(\"./images/defect_freedom_res.png\", dpi = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_def = []\n",
    "for i in defect_bw:\n",
    "    i_mc = i.reshape((m*n,), order = 'C') - avg_fanc\n",
    "    partial = U[:,:rank].T @ i_mc\n",
    "    rec = avg_fanc + U[:,:rank] @ partial\n",
    "    rec = rec.reshape((m, n), order = 'C')\n",
    "    res = rec - i\n",
    "    residuals_def.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defect images\n",
    "fig, ax = plt.subplots(nrows = 4, ncols = 5, figsize = (15,15))\n",
    "plt.tight_layout()\n",
    "for a, i in zip (ax.flat, residuals_def):\n",
    "    a.imshow(i, cmap = 'gray')\n",
    "fig.suptitle(\"Defective Images Residuals\", fontproperties = {'size':24, 'family':'serif'}, y=1.005);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_def_free = []\n",
    "for i in defect_free_bw:\n",
    "    i_mc = i.reshape((m*n,), order = 'C') - avg_fanc\n",
    "    partial = U[:,:rank].T @ i_mc\n",
    "    rec = avg_fanc + U[:,:rank] @ partial\n",
    "    rec = rec.reshape((m, n), order = 'C')\n",
    "    res = rec - i\n",
    "    residuals_def_free.append(res)\n",
    "\n",
    "    residuals_def_free = []\n",
    "for i in defect_free_bw:\n",
    "    i_mc = i.reshape((m*n,), order = 'C') - avg_fanc\n",
    "    partial = U[:,:rank].T @ i_mc\n",
    "    rec = avg_fanc + U[:,:rank] @ partial\n",
    "    rec = rec.reshape((m, n), order = 'C')\n",
    "    res = rec - i\n",
    "    residuals_def_free.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_statistics(array, name, w, dpi=150, plot = True, save = True):\n",
    "    print(\"=======\\nSummary of array:\\n\")\n",
    "    print(\"Mean:\", array.mean())\n",
    "    print(\"Std:\", array.std())\n",
    "    print(\"Max:\", array.max())\n",
    "    print(\"Min:\", array.min())\n",
    "    print(\"\\n=======\\n\\n\")\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        \n",
    "        gs = fig.add_gridspec(1, 2,  width_ratios=(7, 2), left=0.1, \n",
    "                              right=0.9, bottom=0.1, top=0.9,\n",
    "                              wspace=0.05, hspace=0.05)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ax = fig.add_subplot(gs[0])\n",
    "        ax_hist = fig.add_subplot(gs[1], sharey=ax)\n",
    "        ax_hist.tick_params(axis=\"x\", labelbottom=False)\n",
    "        ax_hist.tick_params(axis=\"y\", labelleft=False)\n",
    "        \n",
    "        ax.scatter(np.arange(array.shape[0]), array, color = 'steelblue', s=50)\n",
    "        \n",
    "        binwidth = w\n",
    "        xymax = np.max(np.abs(array))\n",
    "        lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "        bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "        ax_hist.hist(array, bins=bins, orientation='horizontal', color = 'steelblue')\n",
    "        fig.suptitle(\"Marginal scatterplot\", fontproperties = {'size':16, 'family':'serif'})\n",
    "    if save:\n",
    "        if plot:\n",
    "            plt.savefig(\"./images/scatter_marginal_\" + name + '.png', dpi = dpi)\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise Exception(\"Error, check plot and save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt. imshow(residuals_def_free[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "check_normality(boxcox(residuals_def_free[0].flatten(), lmbda=.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics(residuals_def_free_mean, w=.1, name = \"defect_free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_def = []\n",
    "for i in residuals_def:\n",
    "    final_res_def.append(binarize(i))\n",
    "\n",
    "residuals_def_mean = np.empty(20)\n",
    "for j,i in enumerate(final_res_def):\n",
    "    residuals_def_mean[j]=np.mean(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_statistics(residuals_def_mean, name = 'defect', w=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def check_normality(array):\n",
    "    s, p = stats.normaltest(array)\n",
    "    alpha = 0.05\n",
    "    print(\"=======\\nNormality test: \\n\\nH_0: array is from N(...)\\n\\n\")\n",
    "    print(\"p = {:g}\".format(p))\n",
    "    if p < alpha:\n",
    "        print(\"\\nThe null hypothesis can be rejected\")\n",
    "    else:\n",
    "        print(\"\\nThe null hypothesis cannot be rejected\")\n",
    "    print(\"\\n=======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the control chart we have to check the normality of the mean residual of defect-free images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normality(residuals_def_free_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building control chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build control line\n",
    "CL = residuals_def_free_mean.mean()\n",
    "UCL = residuals_def_free_mean.mean() + 3 * residuals_def_free_mean.std()\n",
    "LCL = residuals_def_free_mean.mean() - 3 * residuals_def_free_mean.std()\n",
    "print(f\"\\n=======\\nCL:{CL:6}\\nUCL:{UCL:6}\\nLCL:{LCL:6}\\n=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape for image\n",
    "m = 230\n",
    "n = 230\n",
    "\n",
    "path = '../data/processed/train/'\n",
    "target = list(pd.read_csv('../data/raw/target.csv', sep = ';').Target)\n",
    "images = []\n",
    "rejected = []\n",
    "for f in range(1,51):\n",
    "    try:\n",
    "        img = cv.imread(path + str(f) + '.bmp', cv.IMREAD_GRAYSCALE)\n",
    "        img = cv.resize(img, [m, n])\n",
    "        images.append(img)\n",
    "    except:\n",
    "        rejected.append(img)\n",
    "print(\"Rejected:\", len(rejected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30520\\4234727100.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimg_preprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimg_preprocessed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "img_preprocessed = []\n",
    "for img in images:\n",
    "    img_preprocessed.append(pipeline(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residuals(img, U=U, S=S, vt=vt, avg_fanc=avg_fanc, rank=rank, m=m, n=n):\n",
    "    i_mc = img.reshape((m*n,), order = 'C') - avg_fanc\n",
    "    partial = U[:,:rank].T @ i_mc\n",
    "    rec = avg_fanc + U[:,:rank] @ partial\n",
    "    rec = rec.reshape((m, n), order = 'C')\n",
    "    res = rec - img\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = []\n",
    "for i in img_preprocessed:\n",
    "    residuals.append(compute_residuals(i))\n",
    "\n",
    "residuals_bw = []\n",
    "for i in residuals:\n",
    "    residuals_bw.append(binarize(i))\n",
    "\n",
    "residuals_mean = []\n",
    "for i in residuals_bw:\n",
    "    residuals_mean.append(np.mean(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.scatterplot(x = np.arange(len(residuals)), y = residuals_mean, hue = target, legend = False)\n",
    "plt.legend(title='Smoker', loc='upper left', labels=['Hell Yeh', 'Nah Bruh'])\n",
    "plt.axhline(CL)\n",
    "plt.axhline(UCL)\n",
    "plt.show(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qda_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
